{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89f5bed1-1fa2-4e9a-b0c1-094ae9c027e2",
   "metadata": {},
   "source": [
    "# Q1. What is an ensemble technique in machine learning?\n",
    "In machine learning, an ensemble technique is a method of combining multiple individual models to create a more robust and accurate model. The basic idea behind ensemble methods is that by combining the predictions of multiple models, the overall performance can be improved compared to using any individual model alone.\n",
    "\n",
    "Ensemble techniques can be applied to various types of machine learning models, including decision trees, support vector machines, neural networks, and more. The two main types of ensemble methods are:\n",
    "\n",
    "1. **Bagging (Bootstrap Aggregating):**\n",
    "   - Bagging involves training multiple instances of the same learning algorithm on different subsets of the training data. Each model in the ensemble is trained independently, and their predictions are typically combined by averaging (for regression problems) or by voting (for classification problems).\n",
    "   - The training subsets are created through bootstrap sampling, where each subset is randomly sampled with replacement from the original training data.\n",
    "   - Examples of bagging algorithms include Random Forest for decision trees.\n",
    "\n",
    "2. **Boosting:**\n",
    "   - Boosting focuses on training a sequence of weak learners (models that are slightly better than random guessing) and giving more weight to examples that are misclassified by the previous models in the sequence.\n",
    "   - The final prediction is typically a weighted sum of the individual weak learner predictions.\n",
    "   - Examples of boosting algorithms include AdaBoost, Gradient Boosting, and XGBoost.\n",
    "\n",
    "Ensemble techniques offer several advantages, including improved generalization performance, increased robustness, and better handling of complex relationships in the data. They are particularly useful when individual models have complementary strengths and weaknesses.\n",
    "\n",
    "Popular ensemble methods include Random Forests, AdaBoost, Gradient Boosting Machines (GBM), XGBoost, and Stacking. Each of these methods has its own variations and parameters that can be tuned to achieve optimal performance for specific tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a52fa71-0309-4d0c-a4c6-b81aae73b155",
   "metadata": {},
   "source": [
    "# Q2. Why are ensemble techniques used in machine learning?\n",
    "Ensemble techniques are used in machine learning for several reasons, and they offer several advantages that contribute to improved model performance and robustness. Here are some key reasons why ensemble techniques are widely used:\r\n",
    "\r\n",
    "1. **Improved Generalization:**\r\n",
    "   - Ensemble methods often lead to improved generalization performance by reducing overfitting. Individual models might overfit the training data, but by combining multiple models, the ensemble tends to capture more robust patterns in the data.\r\n",
    "\r\n",
    "2. **Increased Robustness:**\r\n",
    "   - Ensembles are more robust to noise and outliers in the data. Outliers or misclassified examples in one model are less likely to significantly impact the overall prediction when combined with predictions from other models.\r\n",
    "\r\n",
    "3. **Reduction of Variance:**\r\n",
    "   - Ensemble methods, especially bagging techniques like Random Forests, help reduce variance in the predictions. By training multiple models on different subsets of the data, the overall variance is decreased, leading to more stable and reliable predictions.\r\n",
    "\r\n",
    "4. **Handling Complex Relationships:**\r\n",
    "   - Ensembles are effective in capturing complex relationships and patterns in the data. By combining the strengths of different models, ensembles can better handle intricate and non-linear relationships that may be challenging for individual models.\r\n",
    "\r\n",
    "5. **Mitigation of Model Bias:**\r\n",
    "   - Ensembles can help mitigate the bias of individual models. If models have different sources of bias, combining them can lead to a more balanced and less biased prediction.\r\n",
    "\r\n",
    "6. **Versatility Across Algorithms:**\r\n",
    "   - Ensemble techniques are versatile and can be applied to a wide range of machine learning algorithms. Whether it's decision trees, support vector machines, or neural networks, ensembles can be constructed to enhance the performance of various base models.\r\n",
    "\r\n",
    "7. **Adaptability to Diverse Data:**\r\n",
    "   - Ensembles are capable of adapting to diverse types of data and learning tasks. They can be effective for both classification and regression problems, making them versatile in different domains.\r\n",
    "\r\n",
    "8. **Easy Parallelization:**\r\n",
    "   - Many ensemble algorithms, especially bagging methods, can be easily parallelized. This makes them computationally efficient, especially when dealing with large datasets.\r\n",
    "\r\n",
    "9. **State-of-the-Art Performance:**\r\n",
    "   - Ensemble methods, particularly advanced algorithms like Gradient Boosting and XGBoost, have demonstrated state-of-the-art performance in various machine learning competitions and real-world applications.\r\n",
    "\r\n",
    "Overall, ensemble techniques are a powerful tool in the machine learning toolkit, providing a practical means to enhance model performance and address challenges associated with individual models. The choice of the specific ensemble method often depends on the nature of the data and the underlying characteristics of the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e67e862-f818-48b1-b48c-3605df7171ed",
   "metadata": {},
   "source": [
    "# Q3. What is bagging?\n",
    "Bagging, which stands for Bootstrap Aggregating, is an ensemble technique in machine learning that involves training multiple instances of the same learning algorithm on different subsets of the training data and then combining their predictions. The primary goal of bagging is to reduce overfitting and improve the generalization performance of the model.\r\n",
    "\r\n",
    "Here's a step-by-step explanation of how bagging works:\r\n",
    "\r\n",
    "1. **Bootstrap Sampling:**\r\n",
    "   - Bagging starts by creating multiple subsets of the original training data through a process called bootstrap sampling.\r\n",
    "   - Bootstrap sampling involves randomly sampling with replacement from the original training dataset to create multiple subsets. Each subset has the same size as the original dataset, but individual examples may be duplicated or omitted.\r\n",
    "\r\n",
    "2. **Model Training:**\r\n",
    "   - For each subset (also called a \"bag\"), a model is trained independently using the chosen learning algorithm. Since the subsets are created through random sampling with replacement, each model sees a slightly different version of the training data.\r\n",
    "\r\n",
    "3. **Model Independence:**\r\n",
    "   - The independence of the models is crucial to the effectiveness of bagging. Because each model is trained on a different subset, they are likely to make different errors and capture different patterns in the data.\r\n",
    "\r\n",
    "4. **Combining Predictions:**\r\n",
    "   - Once all models are trained, their predictions are combined to make the final prediction. The combination process varies depending on whether the problem is a classification or regression task.\r\n",
    "     - For classification tasks, a common approach is to use a majority voting scheme. The final prediction is the class that receives the most votes from the individual models.\r\n",
    "     - For regression tasks, the final prediction is often the average or median of the individual model predictions.\r\n",
    "\r\n",
    "5. **Reducing Variance:**\r\n",
    "   - The key benefit of bagging is that it helps reduce the variance of the model. By training models on different subsets of the data, the overall model is less sensitive to noise or outliers in any particular subset.\r\n",
    "\r\n",
    "6. **Random Forests:**\r\n",
    "   - One of the most well-known and widely used bagging algorithms is the Random Forest algorithm. It applies bagging to decision trees, where each tree is trained on a different subset of the data, and their predictions are combined through majority voting.\r\n",
    "\r\n",
    "Bagging is a versatile technique and can be applied to various learning algorithms, not just decision trees. It is an effective strategy for creating stable and robust models, especially in situations where individual models may overfit the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffc1219-0cdd-4f3b-9d84-24b69d6b5f21",
   "metadata": {},
   "source": [
    "# Q4. What is boosting?\n",
    "Boosting is another ensemble technique in machine learning that aims to improve the performance of weak learners (models that are only slightly better than random guessing) by combining them in a sequential manner. Unlike bagging, which trains models independently and in parallel, boosting trains models sequentially, with each new model focusing on correcting the errors of its predecessors. The primary goal of boosting is to create a strong learner that performs well on the overall dataset.\n",
    "\n",
    "Here's a step-by-step explanation of how boosting works:\n",
    "\n",
    "1. **Base Model Training:**\n",
    "   - Boosting starts by training a base model (weak learner) on the entire training dataset.\n",
    "\n",
    "2. **Weighting Examples:**\n",
    "   - After the first model is trained, each example in the training dataset is assigned a weight based on its classification error. Misclassified examples are given higher weights, while correctly classified examples receive lower weights.\n",
    "\n",
    "3. **Sequential Model Training:**\n",
    "   - A new model is trained on the dataset, with more emphasis given to examples that were misclassified by the previous models. This process is repeated for a predefined number of iterations or until a performance threshold is reached.\n",
    "\n",
    "4. **Weighted Voting:**\n",
    "   - At each step, the predictions of all models are combined, but the contribution of each model is weighted based on its performance on the training data. Models that perform well are given higher influence in the final prediction.\n",
    "\n",
    "5. **Adaptive Learning:**\n",
    "   - The boosting algorithm adapts by adjusting the weights of the examples at each iteration. Examples that are consistently misclassified receive higher weights, forcing subsequent models to focus more on getting these examples correct.\n",
    "\n",
    "6. **Final Prediction:**\n",
    "   - The final prediction is typically a weighted sum of the predictions of all the weak learners. In classification problems, a common approach is to use a weighted majority voting scheme, and in regression problems, a weighted average is often used.\n",
    "\n",
    "Popular boosting algorithms include:\n",
    "\n",
    "- **AdaBoost (Adaptive Boosting):** One of the earliest and most well-known boosting algorithms. It assigns weights to examples and focuses on misclassified examples in subsequent iterations.\n",
    "  \n",
    "- **Gradient Boosting:** A more general boosting algorithm that minimizes a loss function by adding weak learners sequentially. It uses gradient descent optimization to update the model at each iteration.\n",
    "\n",
    "- **XGBoost (Extreme Gradient Boosting):** An efficient and scalable implementation of gradient boosting that has become popular for its high performance and flexibility.\n",
    "\n",
    "Boosting is powerful but can be sensitive to noise and outliers in the data. It is important to tune hyperparameters carefully to prevent overfitting, and boosting algorithms often perform well when combined with decision trees as base models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c50ef10-1c52-4221-8bdf-69973644fbcb",
   "metadata": {},
   "source": [
    "# Q5. What are the benefits of using ensemble techniques?\n",
    "Ensemble techniques offer several benefits in machine learning, contributing to improved model performance, robustness, and generalization. Here are some key benefits of using ensemble techniques:\n",
    "\n",
    "1. **Improved Accuracy:**\n",
    "   - Ensembles often achieve higher accuracy than individual models. By combining multiple models, ensemble methods can compensate for the weaknesses of individual models and make more accurate predictions.\n",
    "\n",
    "2. **Robustness to Noise:**\n",
    "   - Ensembles are more robust to noise and outliers in the data. Outliers or misclassified examples in one model are less likely to significantly impact the overall prediction when combined with predictions from other models.\n",
    "\n",
    "3. **Reduced Overfitting:**\n",
    "   - Ensemble techniques, especially bagging, help reduce overfitting by combining predictions from multiple models that are trained on different subsets of the data. This results in a more generalized model that performs well on unseen data.\n",
    "\n",
    "4. **Handling Complexity:**\n",
    "   - Ensembles are capable of capturing complex relationships and patterns in the data. By combining the strengths of different models, ensembles can better handle intricate and non-linear relationships that may be challenging for individual models.\n",
    "\n",
    "5. **Versatility Across Algorithms:**\n",
    "   - Ensemble techniques can be applied to various machine learning algorithms, making them versatile. Whether it's decision trees, support vector machines, or neural networks, ensembles can be constructed to enhance the performance of different base models.\n",
    "\n",
    "6. **Adaptability to Diverse Data:**\n",
    "   - Ensembles can adapt well to diverse types of data and learning tasks. They are effective for both classification and regression problems, making them suitable for a wide range of applications.\n",
    "\n",
    "7. **Interpretability:**\n",
    "   - While individual models might be complex and difficult to interpret, the combination of simpler models in an ensemble can result in a more interpretable overall model. This is especially true for ensembles like Random Forests.\n",
    "\n",
    "8. **Ease of Implementation:**\n",
    "   - Ensembles are relatively easy to implement, especially with the availability of libraries and frameworks that provide pre-built implementations of popular ensemble algorithms. This allows practitioners to harness the power of ensembles without extensive manual effort.\n",
    "\n",
    "9. **State-of-the-Art Performance:**\n",
    "   - In various machine learning competitions and real-world applications, ensemble methods, particularly advanced algorithms like Gradient Boosting and XGBoost, have demonstrated state-of-the-art performance.\n",
    "\n",
    "10. **Reduction of Variance:**\n",
    "    - Ensemble methods, especially bagging, help reduce the variance of the model. By training models on different subsets of the data, the overall model is less sensitive to noise or outliers in any particular subset.\n",
    "\n",
    "11. **Parallelization:**\n",
    "    - Many ensemble algorithms, especially bagging methods, can be easily parallelized. This makes them computationally efficient, especially when dealing with large datasets.\n",
    "\n",
    "In summary, ensemble techniques provide a powerful and effective approach to enhancing machine learning models. Their ability to combine the strengths of different models and mitigate their weaknesses makes them a valuable tool in improving model robustness and predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d47b0d8-8c57-493d-b7aa-466d76bd2dbd",
   "metadata": {},
   "source": [
    "# Q6. Are ensemble techniques always better than individual models?\n",
    "While ensemble techniques often lead to improved performance compared to individual models, it's not always the case that ensembles are better in every scenario. The effectiveness of ensemble techniques depends on various factors, and there are situations where individual models may perform as well as, or even better than, ensembles. Here are some considerations:\n",
    "\n",
    "1. **Data Quality:**\n",
    "   - If the training data is of poor quality, noisy, or contains irrelevant features, ensembles might also capture these issues. In such cases, cleaning and preprocessing the data might be more crucial than relying on ensembles.\n",
    "\n",
    "2. **Model Complexity:**\n",
    "   - If the individual models in the ensemble are already complex and overfitted to the training data, combining them may not lead to significant improvements. Ensemble techniques are particularly effective when the base models are diverse and capture different aspects of the data.\n",
    "\n",
    "3. **Computational Resources:**\n",
    "   - Ensembles can be computationally more expensive than training and deploying a single model. In scenarios where computational resources are limited, and there are constraints on model deployment, the overhead of maintaining an ensemble might not be justified.\n",
    "\n",
    "4. **Interpretability:**\n",
    "   - Individual models might be more interpretable than ensembles, especially when transparency and explainability are crucial. If interpretability is a primary concern, a single, interpretable model might be preferred.\n",
    "\n",
    "5. **Applicability of Ensemble Methods:**\n",
    "   - Ensemble techniques are generally beneficial when there is variance in the training data, and different models can capture different patterns. In cases where there is little variance or patterns are straightforward, the benefits of ensembles might be limited.\n",
    "\n",
    "6. **Training Data Size:**\n",
    "   - In situations where the training dataset is small, ensembles might not provide a substantial advantage. Ensemble techniques often shine when there is enough diverse data to train different models effectively.\n",
    "\n",
    "7. **Choice of Ensemble Algorithm:**\n",
    "   - The choice of the ensemble algorithm matters. Some problems might be better suited for bagging methods (e.g., Random Forests), while others might benefit more from boosting methods (e.g., AdaBoost, XGBoost). The selection depends on the characteristics of the data and the learning task.\n",
    "\n",
    "8. **Task Specifics:**\n",
    "   - The nature of the learning task (classification, regression, etc.) and the specific requirements of the application can influence whether ensembles are the best choice. For example, in certain scenarios, a simpler model might be sufficient.\n",
    "\n",
    "9. **Hyperparameter Tuning:**\n",
    "   - Ensembles often require careful hyperparameter tuning to achieve optimal performance. In some cases, improper tuning may lead to suboptimal results, and individual models with well-tuned hyperparameters might perform better.\n",
    "\n",
    "In summary, while ensemble techniques are powerful tools in machine learning, their superiority over individual models is not guaranteed for every problem. The choice between using an ensemble or an individual model depends on the characteristics of the data, the complexity of the problem, and the specific goals of the modeling task. It's advisable to experiment with both approaches and carefully evaluate the performance in the context of the given problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9924c8ff-2a45-4664-ba84-ca0ec7b87a87",
   "metadata": {},
   "source": [
    "# Q7. How is the confidence interval calculated using bootstrap?\n",
    "\n",
    "Bootstrap is a resampling technique used in statistics to estimate the distribution of a statistic by resampling with replacement from the observed data. The confidence interval using bootstrap is calculated by constructing intervals from the distribution of a sample statistic obtained through multiple bootstrap samples. Here are the steps to calculate a bootstrap confidence interval:\n",
    "\n",
    "Collect Data:\n",
    "\n",
    "Obtain the original dataset with observed values.\n",
    "Resample with Replacement:\n",
    "\n",
    "Generate multiple bootstrap samples by randomly selecting data points from the original dataset with replacement. Each bootstrap sample has the same size as the original dataset.\n",
    "Calculate Statistic:\n",
    "\n",
    "For each bootstrap sample, calculate the statistic of interest (e.g., mean, median, standard deviation, etc.).\n",
    "Obtain Bootstrap Distribution:\n",
    "\n",
    "Create a distribution of the calculated statistic from the multiple bootstrap samples.\n",
    "Calculate Confidence Interval:\n",
    "\n",
    "Determine the confidence interval by finding the range of values that encloses a specified percentage of the bootstrap distribution. The most common confidence intervals are the two-tailed intervals, such as the 95% confidence interval.\n",
    "\n",
    "For a symmetric interval (e.g., 95% confidence interval), you can find the lower and upper bounds by taking the values at the 2.5th and 97.5th percentiles of the bootstrap distribution, respectively.\n",
    "\n",
    "Lower Bound\n",
    "=\n",
    "2.5th Percentile of Bootstrap Distribution\n",
    "Lower Bound=2.5th Percentile of Bootstrap Distribution\n",
    "Upper Bound\n",
    "=\n",
    "97.5th Percentile of Bootstrap Distribution\n",
    "Upper Bound=97.5th Percentile of Bootstrap Distribution\n",
    "\n",
    "For an asymmetric interval (e.g., if the bootstrap distribution is skewed), you can calculate percentiles accordingly.\n",
    "\n",
    "Here is a simplified example in Python to illustrate the process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dbe2213-74e5-4ee3-94cb-1a104e42334d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% Confidence Interval: [3.70, 7.20]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Original data\n",
    "original_data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "# Number of bootstrap samples\n",
    "num_samples = 1000\n",
    "\n",
    "# Bootstrap resampling\n",
    "bootstrap_samples = [np.random.choice(original_data, size=len(original_data), replace=True) for _ in range(num_samples)]\n",
    "\n",
    "# Calculate mean for each bootstrap sample\n",
    "bootstrap_means = np.mean(bootstrap_samples, axis=1)\n",
    "\n",
    "# Calculate 95% confidence interval\n",
    "lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "print(f\"95% Confidence Interval: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07915875-7197-4403-b789-6655bc65a6e9",
   "metadata": {},
   "source": [
    "# Q8. How does bootstrap work and What are the steps involved in bootstrap?\n",
    "Bootstrap is a statistical resampling technique used to estimate the sampling distribution of a statistic by repeatedly resampling with replacement from the observed data. The primary goal is to infer properties of the population distribution without making strong parametric assumptions. Here are the steps involved in the bootstrap procedure:\n",
    "\n",
    "Original Data:\n",
    "\n",
    "Start with a dataset containing observed values. Let's denote this dataset as \n",
    "�\n",
    "X with \n",
    "�\n",
    "n data points.\n",
    "Resample with Replacement:\n",
    "\n",
    "Generate multiple bootstrap samples by randomly selecting \n",
    "�\n",
    "n data points from the original dataset with replacement. This means that each data point is equally likely to be selected in each draw, and some data points may be selected multiple times.\n",
    "Calculate Statistic:\n",
    "\n",
    "For each bootstrap sample, calculate the statistic of interest. This statistic could be the mean, median, standard deviation, variance, or any other measure that characterizes the distribution.\n",
    "Repeat Steps 2-3:\n",
    "\n",
    "Repeat the resampling and calculation process a large number of times (typically thousands or more) to obtain a collection of bootstrap samples and the corresponding statistics.\n",
    "Estimate Population Distribution:\n",
    "\n",
    "Use the distribution of the calculated statistics from the bootstrap samples to estimate the sampling distribution of the statistic. This distribution provides information about the variability of the statistic and can be used to construct confidence intervals or perform hypothesis testing.\n",
    "The key idea behind bootstrap is that, under certain conditions, the distribution of the statistic calculated from the bootstrap samples can be considered an approximation of the sampling distribution of the statistic in the population.\n",
    "\n",
    "Here's a simplified example in Python to illustrate the steps of bootstrap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "069a2c58-4c07-4f3d-929e-4204441048f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Original data\n",
    "original_data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "# Number of bootstrap samples\n",
    "num_samples = 1000\n",
    "\n",
    "# Bootstrap resampling\n",
    "bootstrap_samples = [np.random.choice(original_data, size=len(original_data), replace=True) for _ in range(num_samples)]\n",
    "\n",
    "# Calculate mean for each bootstrap sample\n",
    "bootstrap_means = np.mean(bootstrap_samples, axis=1)\n",
    "\n",
    "# Example: Plot histogram of bootstrap means\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(bootstrap_means, bins=30, edgecolor='black')\n",
    "plt.title('Bootstrap Sampling Distribution of the Mean')\n",
    "plt.xlabel('Sample Mean')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfddcf09-5adc-483b-9d19-99fb07a7c0c2",
   "metadata": {},
   "source": [
    "# Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of asample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use bootstrap to estimate the 95% confidence interval for the population mean height.\n",
    "To estimate the 95% confidence interval for the population mean height using bootstrap, we can follow these steps:\r\n",
    "\r\n",
    "Original Data:\r\n",
    "\r\n",
    "The original data consists of the sample of 50 tree heights with a mean of 15 meters and a standard deviation of 2 meters.\r\n",
    "Resample with Replacement:\r\n",
    "\r\n",
    "Generate multiple bootstrap samples by randomly selecting 50 tree heights from the original sample with replacement.\r\n",
    "Calculate Bootstrap Mean:\r\n",
    "\r\n",
    "For each bootstrap sample, calculate the mean height.\r\n",
    "Repeat Steps 2-3:\r\n",
    "\r\n",
    "Repeat the resampling and calculation process a large number of times (e.g., 10,000 times) to obtain a distribution of bootstrap sample means.\r\n",
    "Calculate Confidence Interval:\r\n",
    "\r\n",
    "Determine the 95% confidence interval by finding the 2.5th and 97.5th percentiles of the distribution of bootstrap sample means.\r\n",
    "Let's implemen\n",
    "t this in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff6e6f28-9dc8-4c12-9486-30fa88179a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% Confidence Interval for the Mean Height: [14.48, 15.64] meters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "# Original sample data\r\n",
    "original_data = np.random.normal(loc=15, scale=2, size=50)  # Simulating tree heights\r\n",
    "\r\n",
    "# Number of bootstrap samples\r\n",
    "num_samples = 10000\r\n",
    "\r\n",
    "# Bootstrap resampling\r\n",
    "bootstrap_means = [np.mean(np.random.choice(original_data, size=50, replace=True)) for _ in range(num_samples)]\r\n",
    "\r\n",
    "# Calculate 95% confidence interval\r\n",
    "confidence_interval = np.percentile(bootstrap_means, [2.5, 97.5])\r\n",
    "\r\n",
    "print(f\"95% Confidence Interval for the Mean Height: [{confidence_interval[0]:.2f}, {confidence_interval[1]:.2f}] meters\")\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38a3d0d-de1b-43b6-a96f-1d8eb7ab783d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a2ce54-899a-46e2-a03a-b482876b6f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dcf883-d339-4878-aa23-13c723d3c1b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0813ce3-5f9e-4901-adf0-e947eb61fabc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb4aaed-fc3a-4a6c-a4e2-51dddb01313d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f230bc9e-caed-4c78-8cd8-a270b0d18458",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfd0f65-2aaf-4d12-9b26-2e78624f1381",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c05fa1c-75f5-4ee0-bef2-c4089f4eb9fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c732e8-9d91-45e9-8fe7-ebb46a59b762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2f9222-fb86-470b-8820-a23e2b63a5ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b824d26-2b4d-4dd2-872c-ba97b1abfcd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78156f6-2db7-49e0-af65-09cfb4731234",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8ea494-3d00-4110-ac68-cc2ff72762bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b81726-e929-45f2-8330-caacbf59c4af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb930f68-ea14-4bc1-b299-e68992d210c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea6a266-2d5d-4ee7-8db5-e92d61888614",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15f39fd-09d2-41bd-afa4-fd2b7104e671",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72f2818-91f5-42a2-ab65-0d7ca283363c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8194acf4-3523-4975-a747-c1c459227529",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81bf831-26a8-42b2-83f6-7200105c9426",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075d37ed-3e60-47ab-a37a-1f435b09d724",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa83f1b-0372-4371-8ad9-8c09178403fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d443dca2-84d7-4565-a67f-ccb623f8af5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458c2995-0d67-4e69-863f-45b4f3cd68d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f4d341-0196-4360-8133-10945d74da9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d770101-b8fd-4217-a6cf-2afa4eb03314",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b916ac-e259-4d55-b904-30afde900fab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc787cce-fb3c-4666-9dc7-31e22ff272a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
